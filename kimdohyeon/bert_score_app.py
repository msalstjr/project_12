import os
import gradio as gr
import pandas as pd
import matplotlib.pyplot as plt
from dotenv import load_dotenv
from bert_score import score

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

from anthropic import Anthropic

# ğŸ”¹ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

# ğŸ”¹ Claude ë©”ì‹œì§€ ê¸°ë°˜ ì‘ë‹µ í•¨ìˆ˜
anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)

def claude_chat_message(prompt, model="claude-3-opus-20240229"):
    response = anthropic_client.messages.create(
        model=model,
        max_tokens=1024,
        temperature=0.2,
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    return response.content[0].text

# ğŸ”¹ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
def load_and_store_pdf(file_path, persist_dir="vector_store"):
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    split_docs = splitter.split_documents(docs)
    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-3-small")
    vectordb = Chroma.from_documents(split_docs, embedding, persist_directory=persist_dir)
    vectordb.persist()
    return vectordb

# ğŸ”¹ RAG ì²´ì¸ ìƒì„±
def get_rag_chain(model_name="gpt-3.5-turbo"):
    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-3-small")
    vectordb = Chroma(persist_directory="vector_store", embedding_function=embedding)
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})
    llm = ChatOpenAI(model_name=model_name, temperature=0.2, openai_api_key=OPENAI_API_KEY)
    chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=False)
    return chain

# ğŸ”¹ ëª¨ë¸ë³„ LLM + RAG ì‘ë‹µ ìƒì„±
def compare_all_models(question):
    results = {}

    # ğŸ”¸ GPT-3.5
    llm_35 = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)
    results['gpt-3.5-LLM'] = llm_35.predict(question)
    results['gpt-3.5-RAG'] = get_rag_chain("gpt-3.5-turbo").run(question)

    # ğŸ”¸ GPT-4
    llm_4 = ChatOpenAI(model_name="gpt-4", openai_api_key=OPENAI_API_KEY)
    results['gpt-4-LLM'] = llm_4.predict(question)
    results['gpt-4-RAG'] = get_rag_chain("gpt-4").run(question)

    # ğŸ”¸ Claude 3 Opus
    results['claude-3-LLM'] = claude_chat_message(question, "claude-3-opus-20240229")
    rag_query = get_rag_chain("gpt-3.5-turbo").run(question)
    results['claude-3-RAG'] = claude_chat_message(rag_query, "claude-3-opus-20240229")

    # ğŸ”¸ Claude 4 Opus
    results['claude-4-LLM'] = claude_chat_message(question, "claude-opus-4-20250514")
    results['claude-4-RAG'] = claude_chat_message(rag_query, "claude-opus-4-20250514")

    return results

# ğŸ”¹ BERTScore ì‹œê°í™”
def plot_bert_score(responses, reference):
    model_names = list(responses.keys())
    cand_list = list(responses.values())
    P, R, F1 = score(cand_list, [reference]*len(cand_list), lang='en', verbose=False)
    df = pd.DataFrame({
        "Model": model_names,
        "BERTScore": F1.numpy()
    }).sort_values("BERTScore", ascending=False)

    plt.figure(figsize=(10, 5))
    plt.bar(df["Model"], df["BERTScore"])
    plt.xticks(rotation=45)
    plt.title("BERTScore (F1) for LLM ì‘ë‹µ")
    plt.ylim(0, 1)
    plt.tight_layout()
    path = "bert_score.png"
    plt.savefig(path)
    return path

# ğŸ”¹ Gradioìš© ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜
def process(pdf_file, question):
    with open("upload.pdf", "wb") as f:
        f.write(pdf_file)

    load_and_store_pdf("upload.pdf")

    responses = compare_all_models(question)

    # LLM ì‘ë‹µë§Œ BERTScore í‰ê°€
    llm_only = {k: v for k, v in responses.items() if "LLM" in k}
    score_plot_path = plot_bert_score(llm_only, question)

    # ì „ì²´ ì‘ë‹µ ì¶œë ¥ ì •ë¦¬
    output_text = "\n\n".join([f"ğŸ”¹ {k}:\n{v}" for k, v in responses.items()])
    return output_text, score_plot_path

# ğŸ”¹ Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ¤– PDF ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ (GPT & Claude + BERTScore ì‹œê°í™”)")
    with gr.Row():
        pdf_input = gr.File(label="PDF ì—…ë¡œë“œ", type="binary")
        question_input = gr.Textbox(label="ì§ˆë¬¸", placeholder="ì˜ˆ: ì•”ë³´í—˜ ë©´ì±…ê¸°ê°„ì€ ëª‡ ì¼ì¸ê°€ìš”?")
    with gr.Row():
        btn = gr.Button("ì§ˆë¬¸í•˜ê¸°")
    with gr.Row():
        response_text = gr.Textbox(label="ëª¨ë¸ ì‘ë‹µ ëª¨ìŒ", lines=20)
    with gr.Row():
        image_output = gr.Image(label="BERTScore ê·¸ë˜í”„")

    btn.click(fn=process, inputs=[pdf_input, question_input], outputs=[response_text, image_output])

# ğŸ”¹ ì‹¤í–‰
if __name__ == "__main__":
    demo.launch()
