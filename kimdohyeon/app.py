import os
import gradio as gr
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
print("OpenAI API Key:", OPENAI_API_KEY)  # ë””ë²„ê¹…ìš© ì¶œë ¥
# ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ í•¨ìˆ˜
def load_and_store_pdf(file_path, persist_dir="vector_store"):
    loader = PyPDFLoader(file_path)
    docs = loader.load()

    splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    split_docs = splitter.split_documents(docs)

    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-3-small")
    vectordb = Chroma.from_documents(split_docs, embedding, persist_directory=persist_dir)
    vectordb.persist()
    return vectordb

# RAG ì²´ì¸ ìƒì„±
def get_rag_chain(persist_dir="vector_store"):
    embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-3-small")
    vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding)
    retriever = vectordb.as_retriever(search_kwargs={"k": 5})

    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.2, openai_api_key=OPENAI_API_KEY)
    chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        return_source_documents=False
    )
    return chain

# LLM vs RAG ì‘ë‹µ ë¹„êµ
def compare_llm_vs_rag(query, rag_chain):
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.2, openai_api_key=OPENAI_API_KEY)
    llm_response = llm.predict(query)
    rag_response = rag_chain.run(query)

    return llm_response, rag_response

# Gradio ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
def process(pdf_file, question):
    # PDF ì €ì¥
    upload_path = "upload.pdf"
    with open(upload_path, "wb") as f:
        f.write(pdf_file)  # ìˆ˜ì •ëœ ë¶€ë¶„

    # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    load_and_store_pdf(upload_path)

    # RAG ì²´ì¸ ìƒì„±
    rag = get_rag_chain()

    # ì§ˆë¬¸ ì²˜ë¦¬
    llm_response, rag_response = compare_llm_vs_rag(question, rag)

    return f"ğŸ”¹ LLM ì‘ë‹µ:\n{llm_response}", f"ğŸ”¹ RAG ì‘ë‹µ:\n{rag_response}"


# Gradio UI ì •ì˜
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ“„ PDF ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ (LLM vs RAG)")
    with gr.Row():
        pdf_input = gr.File(label="PDF íŒŒì¼ ì—…ë¡œë“œ", type="binary")
        question_input = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ: ìœ ì‚¬ì•”ì§„ë‹¨ë¹„ ì²­êµ¬ ì‹œ ê°‘ìƒì„ ì•” ì§„ë‹¨ ê¸°ì¤€ì€?")
    with gr.Row():
        btn = gr.Button("ì§ˆë¬¸í•˜ê¸°")
    with gr.Row():
        llm_output = gr.Textbox(label="LLM ì‘ë‹µ (GPT-3.5)", lines=10)
        rag_output = gr.Textbox(label="RAG ì‘ë‹µ (ë¬¸ì„œ ê¸°ë°˜)", lines=10)

    btn.click(fn=process, inputs=[pdf_input, question_input], outputs=[llm_output, rag_output])

# ì‹¤í–‰
if __name__ == "__main__":
    demo.launch()
