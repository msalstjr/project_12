import os
import shutil
import gradio as gr
from dotenv import load_dotenv

import fitz  # PyMuPDF
from langchain.document_loaders import TextLoader
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

# 1. í™˜ê²½ì„¤ì • ë° í´ë” ì¤€ë¹„
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

VECTOR_ROOT = "vectorstores"
UPLOAD_DIR = "uploaded_pdfs"
TXT_DIR = "converted_txts"
os.makedirs(VECTOR_ROOT, exist_ok=True)
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(TXT_DIR, exist_ok=True)

uploaded_files = []
built_categories = set()

# 2. ì¹´í…Œê³ ë¦¬ ì •ì˜
category_keywords = {
    "cancer": ["ì•”"],
    "medical": ["ì‹¤ì†"],
    "accident": ["ìƒí•´"],
    "fire": ["í™”ì¬"],
}

def get_category_from_filename(filename):
    for category, keywords in category_keywords.items():
        if any(keyword in filename for keyword in keywords):
            return category
    return "cancer"  # fallback

# 3. PDF â†’ ì¢Œ/ìš° í…ìŠ¤íŠ¸ ë¶„ë¦¬ í•¨ìˆ˜ (cancer ì „ìš©)
def extract_left_right_text(pdf_path):
    doc = fitz.open(pdf_path)
    left_text_total = ""
    right_text_total = ""
    for page_num, page in enumerate(doc):
        width = page.rect.width
        blocks = page.get_text("blocks")
        left_text = ""
        right_text = ""
        for b in blocks:
            x0, y0, x1, y1, text, *_ = b
            center_x = (x0 + x1) / 2
            if center_x < width / 2:
                left_text += text.strip() + " "
            else:
                right_text += text.strip() + " "
        left_text_total += f"[í˜ì´ì§€ {page_num + 1} - ì¢Œ]\n{left_text.strip()}\n\n"
        right_text_total += f"[í˜ì´ì§€ {page_num + 1} - ìš°]\n{right_text.strip()}\n\n"
    return left_text_total + right_text_total

# 4. PDF â†’ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬)
def extract_full_text(pdf_path):
    doc = fitz.open(pdf_path)
    full_text = ""
    for page in doc:
        full_text += page.get_text()
        full_text += "\n\n"
    return full_text

# 5. PDF ì—…ë¡œë“œ ì‹œ TXT ìë™ ë³€í™˜ (ì¹´í…Œê³ ë¦¬ë³„ ë¶„ê¸°)
def upload_pdf(file, file_list):
    filename = os.path.basename(file.name)
    save_path = os.path.join(UPLOAD_DIR, filename)
    txt_path = os.path.join(TXT_DIR, filename.replace(".pdf", ".txt"))

    shutil.copy(file.name, save_path)
    category = get_category_from_filename(filename)
    # cancerë§Œ ì¢Œ/ìš° ë¶„ë¦¬, ë‚˜ë¨¸ì§€ëŠ” ì „ì²´ í…ìŠ¤íŠ¸
    if not os.path.exists(txt_path):
        if category == "cancer":
            txt = extract_left_right_text(save_path)
        else:
            txt = extract_full_text(save_path)
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(txt)
    if save_path not in file_list:
        file_list.append(save_path)
    all_filenames = [os.path.basename(f) for f in file_list]
    return file_list, "\n".join(all_filenames)

# 6. ì²­í¬ ë° ì„ë² ë”©
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
embedding = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=OPENAI_API_KEY,
)

# 7. ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
def build_vectorstores(file_list):
    global built_categories
    built_categories.clear()
    for path in file_list:
        filename = os.path.basename(path)
        category = get_category_from_filename(filename)
        print(f"[+] Building vectorstore for: {filename} â†’ category={category}")
        txt_path = os.path.join(TXT_DIR, filename.replace(".pdf", ".txt"))
        loader = TextLoader(txt_path, encoding="utf-8")
        docs_raw = loader.load()
        docs = text_splitter.split_documents(docs_raw)
        vectorstore_path = os.path.join(VECTOR_ROOT, category)
        if os.path.exists(vectorstore_path):
            shutil.rmtree(vectorstore_path)
        vectorstore = FAISS.from_documents(docs, embedding=embedding)
        vectorstore.save_local(vectorstore_path)
        built_categories.add(category)
    return f"ğŸ“š ì´ {len(built_categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {', '.join(built_categories)}"

# 8. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì§ˆë¬¸ ê¸°ì¤€)
category_keywords_full = {
    "cancer": [
        "ì•”", "ìœ ì‚¬ì•”", "íŠ¹ì •ì•”", "ì „ì´ì•”", "ì¬ì§„ë‹¨ì•”", "ê°‘ìƒì„ ì•”", "íì•”", "ê°„ì•”", "ì·Œì¥ì•”",
        "ì†Œí™”ê¸°ê´€ì•”", "í˜ˆì•¡ì•”", "ìƒì‹ê¸°ì•”", "í•­ì•”", "í•­ì•”ì¹˜ë£Œ", "ë°©ì‚¬ì„ ì¹˜ë£Œ", "í•­ì•”ë°©ì‚¬ì„ ",
        "í•­ì•”ì•½ë¬¼", "í‘œì í•­ì•”", "í˜¸ë¥´ëª¬ì•½ë¬¼", "CAR-T", "ì§„ë‹¨ë¹„", "ì•”ì§„ë‹¨ë¹„", "ì•”ì‚¬ë§"
    ],
    "medical": [
        "ì‹¤ì†", "ì‹¤ë¹„", "ì‹¤ì†ì˜ë£Œë¹„ë³´í—˜"
    ],
    "accident": [
        "ìƒí•´", "ì¬í•´", "êµí†µì‚¬ê³ ", "í›„ìœ ì¥í•´", "ì‚¬ê³ ì‚¬ë§",
        "ìƒí•´ì‚¬ë§", "ìƒí•´íŠ¹ì•½", "ì•ˆì „ë²¨íŠ¸", "ë§ˆë¼í†¤"
    ],
    "fire": [
        "í™”ì¬", "í­ë°œ", "ë¶•ê´´", "ëˆ„ìˆ˜", "ë„ë‚œ", "ë°°ìƒì±…ì„", "ì¬ì‚°", "ê°€ì¬ë„êµ¬",
        "ë³µêµ¬", "ì†í•´", "í”¼í•´", "ì£¼íƒ", "í™”ì¬ë³´í—˜", "í™”ì¬ì‚¬ê³ ", "ìœ ë¦¬ì†í•´"
    ]
}

def classify_question(question):
    for category, keywords in category_keywords_full.items():
        if any(keyword in question for keyword in keywords):
            return category
    return "cancer"

# 9. LLM ë° í”„ë¡¬í”„íŠ¸ ì„¸íŒ…
llm = ChatAnthropic(
    model="claude-opus-4-20250514",
    temperature=0,
    max_tokens=1024,
    api_key=ANTHROPIC_API_KEY,
)
prompt = hub.pull("rlm/rag-prompt")
llm_only_chain = llm | StrOutputParser()

# 10. ì§ˆë¬¸ ë‹µë³€ í•¨ìˆ˜ (RAG)
def answer_question(question, chat_history):
    category = classify_question(question)
    vectorstore_path = os.path.join(VECTOR_ROOT, category)
    if not os.path.exists(vectorstore_path):
        msg = f"âŒ ì„ íƒëœ ì¹´í…Œê³ ë¦¬ '{category}'ì˜ ë²¡í„°ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ê´€ë ¨ PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”."
        chat_history.append((question, msg))
        return chat_history
    vectorstore = FAISS.load_local(vectorstore_path, embeddings=embedding, allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever()
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()} |
        prompt | llm | StrOutputParser()
    )
    rag_answer = rag_chain.invoke(question)
    kor_category = {
        "cancer": "ì•” ë³´í—˜",
        "medical": "ì‹¤ë¹„ ë³´í—˜",
        "accident": "ìƒí•´ ë³´í—˜",
        "fire": "í™”ì¬ë³´í—˜"
    }.get(category, category)
    combined = f"[ğŸ“‚ ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {kor_category}]\n\n"
    combined += f"ğŸ“š RAG ê¸°ë°˜ ì‘ë‹µ:\n{rag_answer}"
    chat_history.append((question, combined))
    return chat_history

# 11. Gradio UI
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ“„ ë³´í—˜ì•½ê´€ RAG ì‹œìŠ¤í…œ")
    with gr.Row():
        file_input = gr.File(label="PDF ì—…ë¡œë“œ", file_types=[".pdf"], file_count="single")
        upload_btn = gr.Button("ğŸ“¥ PDF ì¶”ê°€")
    upload_status = gr.Textbox(label="ì—…ë¡œë“œëœ PDF ëª©ë¡", interactive=False)
    state_files = gr.State([])
    state_chat = gr.State([])
    confirm_btn = gr.Button("âœ… PDF ì—…ë¡œë“œ ì™„ë£Œ & ë²¡í„°ìŠ¤í† ì–´ ìƒì„±")
    confirm_output = gr.Textbox(label="ìƒì„± ìƒíƒœ", interactive=False)
    gr.Markdown("### ğŸ’¬ ë³´í—˜ ì•½ê´€ ì±—ë´‡ (Chat RAG)")
    with gr.Row():
        chatbot = gr.Chatbot(label="ì±—ë´‡ ëŒ€í™” ë‚´ì—­", height=400, show_copy_button=True, avatar_images=(None, None))
    chat_input = gr.Textbox(label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ê¶ê¸ˆí•œ ì ì„ ì…ë ¥ í›„ Enter", lines=1)
    upload_btn.click(
        upload_pdf,
        inputs=[file_input, state_files],
        outputs=[state_files, upload_status]
    )
    confirm_btn.click(
        build_vectorstores,
        inputs=[state_files],
        outputs=[confirm_output]
    )
    chat_input.submit(
        answer_question,
        inputs=[chat_input, state_chat],
        outputs=[chatbot],
        queue=True
    ).then(
        lambda chat_input, chat_history: ("", chat_history),
        inputs=[chat_input, state_chat],
        outputs=[chat_input, state_chat]
    )
demo.queue()
demo.launch()
