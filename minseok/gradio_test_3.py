import os
import shutil
import gradio as gr
import fitz  # PyMuPDF
from typing import List, Tuple
from langchain.document_loaders import TextLoader
from langchain.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_anthropic import ChatAnthropic
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub
from dotenv import load_dotenv

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

VECTOR_ROOT = "vectorstores"
UPLOAD_DIR = "uploaded_pdfs"
os.makedirs(VECTOR_ROOT, exist_ok=True)
os.makedirs(UPLOAD_DIR, exist_ok=True)

uploaded_files = []
built_categories = set()

category_keywords = {"cancer"  : ["ì•”"],
                     "medical" : ["ì‹¤ì†"], 
                     "accident": ["ìƒí•´"],
                     "fire"    : ["í™”ì¬"],
                     }

def get_category_from_filename(filename):
    for category, keywords in category_keywords.items():
        if any(keyword in filename for keyword in keywords):
            return category
    return "cancer"

# === ğŸŸ  PDF ì¢Œ/ìš° í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ì¶”ê°€ ===
def extract_left_right_text(pdf_path):
    doc = fitz.open(pdf_path)
    left_text_total = ""
    right_text_total = ""

    for page_num, page in enumerate(doc):
        width = page.rect.width
        blocks = page.get_text("blocks")

        left_text = ""
        right_text = ""

        for b in blocks:
            x0, y0, x1, y1, text, *_ = b
            center_x = (x0 + x1) / 2
            if center_x < width / 2:
                left_text += text.strip() + " "
            else:
                right_text += text.strip() + " "

        left_text_total += f"[í˜ì´ì§€ {page_num + 1} - ì¢Œ]\n{left_text.strip()}\n\n"
        right_text_total += f"[í˜ì´ì§€ {page_num + 1} - ìš°]\n{right_text.strip()}\n\n"

    return left_text_total + right_text_total

text_splitter = RecursiveCharacterTextSplitter(chunk_size    = 300, 
                                               chunk_overlap = 50,
                                               )
embedding = OpenAIEmbeddings(model="text-embedding-3-small", 
                             openai_api_key=OPENAI_API_KEY,
                             )

# === ğŸŸ  PDF ì—…ë¡œë“œ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ===
def upload_pdf(file, file_list):
    filename = os.path.basename(file.name)
    save_path = os.path.join(UPLOAD_DIR, filename)
    shutil.copy(file.name, save_path)
    if save_path not in file_list:
        file_list.append(save_path)
    all_filenames = [os.path.basename(f) for f in file_list]
    return file_list, "\n".join(all_filenames)

# === ğŸŸ  ë²¡í„°ìŠ¤í† ì–´ ìƒì„± í•¨ìˆ˜ (ì—¬ê¸°ì„œ PDFâ†’txt ìë™ë³€í™˜ í¬í•¨) ===
def build_vectorstores(file_list):
    global built_categories
    built_categories.clear()
    for path in file_list:
        filename = os.path.basename(path)
        category = get_category_from_filename(filename)
        print(f"[+] Building vectorstore for: {filename} â†’ category={category}")

        # 1. PDFâ†’TXT (ì—†ìœ¼ë©´ ìƒì„±)
        txt_path = os.path.splitext(path)[0] + ".txt"
        if not os.path.exists(txt_path):
            full_text = extract_left_right_text(path)
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(full_text)
            print(f"âœ… í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {txt_path}")
        else:
            print(f"ğŸ“„ ê¸°ì¡´ í…ìŠ¤íŠ¸ íŒŒì¼ ì‚¬ìš©: {txt_path}")

        # 2. TXTâ†’ë¬¸ì„œâ†’ì„ë² ë”©â†’ë²¡í„°ìŠ¤í† ì–´
        loader = TextLoader(txt_path, encoding="utf-8")
        docs_raw = loader.load()
        docs = text_splitter.split_documents(docs_raw)

        vectorstore_path = os.path.join(VECTOR_ROOT, category)
        if os.path.exists(vectorstore_path):
            shutil.rmtree(vectorstore_path)
        vectorstore = FAISS.from_documents(docs, embedding=embedding)
        vectorstore.save_local(vectorstore_path)
        built_categories.add(category)

    return f"ğŸ“š ì´ {len(built_categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ: {', '.join(built_categories)}"

# === ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìš© í‚¤ì›Œë“œ ===
category_keywords_full = {
    "cancer": [
        "ì•”", "ìœ ì‚¬ì•”", "íŠ¹ì •ì•”", "ì „ì´ì•”", "ì¬ì§„ë‹¨ì•”", "ê°‘ìƒì„ ì•”", "íì•”", "ê°„ì•”", "ì·Œì¥ì•”",
        "ì†Œí™”ê¸°ê´€ì•”", "í˜ˆì•¡ì•”", "ìƒì‹ê¸°ì•”", "í•­ì•”", "í•­ì•”ì¹˜ë£Œ", "ë°©ì‚¬ì„ ì¹˜ë£Œ", "í•­ì•”ë°©ì‚¬ì„ ",
        "í•­ì•”ì•½ë¬¼", "í‘œì í•­ì•”", "í˜¸ë¥´ëª¬ì•½ë¬¼", "CAR-T", "ì§„ë‹¨ë¹„", "ì•”ì§„ë‹¨ë¹„", "ì•”ì‚¬ë§"
    ],
    "medical": [
        "ì‹¤ì†", "ì˜ë£Œë¹„", "ì…ì›", "í†µì›", "ì§„ë£Œë¹„", "ê²€ì‚¬ë¹„", "ìˆ˜ìˆ ", "ì‘ê¸‰ì‹¤", "ì¹˜ë£Œ",
        "ìê¸°ë¶€ë‹´ê¸ˆ", "ë³´í—˜ê¸ˆ í•œë„", "ì§„ë‹¨ì„œ", "ì˜ë¬´ê¸°ë¡", "ë³´ìƒì¢…ëª©", "ë‹¤ìˆ˜ë³´í—˜", "ì—°ëŒ€ì±…ì„"
    ],
    "accident": [
        "ìƒí•´", "ì¬í•´", "ì‚¬ê³ ", "êµí†µì‚¬ê³ ", "ê³¨ì ˆ", "í™”ìƒ", "í›„ìœ ì¥í•´", "ì‚¬ê³ ì‚¬ë§",
        "ì…ì›ë¹„", "ìˆ˜ìˆ ë¹„", "ìƒí•´ì‚¬ë§", "ìƒí•´íŠ¹ì•½"
    ],
    "fire": [
        "í™”ì¬", "í­ë°œ", "ë¶•ê´´", "ëˆ„ìˆ˜", "ë„ë‚œ", "ë°°ìƒì±…ì„", "ì¬ì‚°", "ê°€ì¬ë„êµ¬",
        "ë³µêµ¬", "ì†í•´", "í”¼í•´", "ì£¼íƒ", "í™”ì¬ë³´í—˜", "í™”ì¬ì‚¬ê³ "
    ]
}

def classify_question(question):
    for category, keywords in category_keywords_full.items():
        if any(keyword in question for keyword in keywords):
            return category
    return "cancer"

llm = ChatAnthropic(model = "claude-opus-4-20250514",
                    temperature = 0,
                    max_tokens = 1024,
                    api_key = ANTHROPIC_API_KEY,
                    )
prompt = hub.pull("rlm/rag-prompt")
llm_only_chain = llm | StrOutputParser()

def answer_question(question, chat_history):
    category = classify_question(question)
    vectorstore_path = os.path.join(VECTOR_ROOT, category)

    if not os.path.exists(vectorstore_path):
        msg = f"âŒ ì„ íƒëœ ì¹´í…Œê³ ë¦¬ '{category}'ì˜ ë²¡í„°ìŠ¤í† ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ê´€ë ¨ PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”."
        chat_history.append((question, msg))
        return chat_history

    vectorstore = FAISS.load_local(vectorstore_path, embeddings=embedding, allow_dangerous_deserialization=True)
    retriever = vectorstore.as_retriever()

    rag_chain = ( {"context": retriever, "question": RunnablePassthrough()} | prompt | llm | StrOutputParser() )
    rag_answer = rag_chain.invoke(question)

    kor_category = ''
    if category == "cancer":
        kor_category = 'ì•” ë³´í—˜'
    elif category == "medical":
        kor_category = "ì‹¤ë¹„ ë³´í—˜"
    elif category == "accident":
        kor_category = "ìƒí•´ ë³´í—˜"
    elif category == "fire":
        kor_category = "í™”ì¬ ë³´í—˜"

    combined = f"[ğŸ“‚ ì„ íƒëœ ì¹´í…Œê³ ë¦¬: {kor_category}]\n\n"
    combined += f"ğŸ“š RAG ê¸°ë°˜ ì‘ë‹µ:\n{rag_answer}"
    chat_history.append((question, combined))
    return chat_history

# === Gradio UI ===
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## ğŸ“„ ë³´í—˜ì•½ê´€ RAG ì‹œìŠ¤í…œ")

    with gr.Row():
        file_input = gr.File(label="PDF ì—…ë¡œë“œ", file_types=[".pdf"], file_count="single")
        upload_btn = gr.Button("ğŸ“¥ PDF ì¶”ê°€")
    upload_status = gr.Textbox(label="ì—…ë¡œë“œëœ PDF ëª©ë¡", interactive=False)

    state_files = gr.State([])
    state_chat = gr.State([])

    confirm_btn = gr.Button("âœ… PDF ì—…ë¡œë“œ ì™„ë£Œ & ë²¡í„°ìŠ¤í† ì–´ ìƒì„±")
    confirm_output = gr.Textbox(label="ìƒì„± ìƒíƒœ", interactive=False)

    gr.Markdown("### ğŸ’¬ ë³´í—˜ ì•½ê´€ ì±—ë´‡ (Chat RAG)")

    with gr.Row():
        chatbot = gr.Chatbot(label="ì±—ë´‡ ëŒ€í™” ë‚´ì—­", height=400, show_copy_button=True, avatar_images=(None, None))

    chat_input = gr.Textbox(label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ê¶ê¸ˆí•œ ì ì„ ì…ë ¥ í›„ Enter", lines=1)

    upload_btn.click(
        upload_pdf, 
        inputs=[file_input, state_files], 
        outputs=[state_files, upload_status]
    )
    confirm_btn.click(
        build_vectorstores,
        inputs=[state_files],
        outputs=[confirm_output]
    )
    chat_input.submit(
        answer_question,
        inputs=[chat_input, state_chat],
        outputs=[chatbot],
        queue=True
    ).then(
        lambda chat_input, chat_history: ("", chat_history),
        inputs=[chat_input, state_chat],
        outputs=[chat_input, state_chat]
    )

demo.queue()
demo.launch()
